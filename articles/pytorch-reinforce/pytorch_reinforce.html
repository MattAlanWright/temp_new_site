<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>A Thinking Rock</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../../styles.css">

    <!-- LaTeX equation support -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <nav>
        <div class="logo">
            <h4>A Thinking Rock</h4>
        </div>
        <ul class="nav-links">
            <li><a href="/">Home</a></li>
            <li><a class="current-page" href="/articles.html">Articles</a></li>
            <li><a href="/about.html">About</a></li>
            <li><a href="/contact.html">Contact</a></li>
        </ul>
        <div class="burger">
            <div class="line1"></div>
            <div class="line2"></div>
            <div class="line3"></div>
        </div>
    </nav>


    <div class="page-contents">
        <div class="article-title">
            <h1>Practical REINFORCE in PyTorch</h1>
        </div>

        <div class="article-body">
            <p>This article is a hands-on introduction to building gradient-based reinforcement learning algorithms in PyTorch. We'll review the policy gradient theorem, the foundation for gradient-based learning methods, and how it's used in practice. Then we'll implement the classic REINFORCE learning algorithm, as it appears in <a href="http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf" target="_blank">Sutton and Barto's Reinforcement Learning</a> and use it to teach an agent to solve the <a href="https://gym.openai.com/envs/CartPole-v0/" target="_blank">OpenAI Gym CartPole environment</a>. This algorithm is a great way to gain experience with gradient-based learning, and understanding it will help pave the way to building more complex learning algorithms, like the Actor-Critic.</p>

            <p>If you've found this page then I'm assuming that you know the basics of reinforcement learning methods and terminology. By the end of this article I hope that you'll have developed an intuition for how gradient-based learning builds on these foundations.</p>

            <div class="article-title">
                <h2>The Policy Gradient Theorem</h2>
            </div>

            <p>Before we look at REINFORCE, let's dive into the policy gradient theorem, which provides the foundation of the algorithm. For the case of an episodic task, the policy gradient theorem as described in Sutton and Barto states that</p>

            <div class="block-eq">
                <div>$$\nabla J(\theta) \propto  \sum_s{\mu(s)}\sum_a{q_{\pi}(s,a)\nabla\pi(a|s,\theta)}$$</div>
            </div>

            <p>where</p>
            <ul>
                <li>\(J(\theta)\) is a scalar performance measure of the current policy</li>
                <li>\(\mu(s)\) is the distribution of states \(s\) over policy \(\pi\)</li>
                <li>\(q_{\pi}(s,a)\) is the action value function evaluated at state \(s\) and action \(a\)</li>
                <li>\(\pi(a|s,\theta)\) is the policy evaluated at state \(s\) and action \(a\), and parameterized by \(\theta\)</li>
            </ul>
        </div>
    </div>

    <script src="../../app.js"></script>
</body>

</html>